# Agentic Course Designer — Multi-Agent FastAPI (No Docker)

This is a minimal multi-agent system built with FastAPI.  
It has four services that run **without Docker**:

- **Orchestrator** (JWT auth, rate-limit, RAI hooks, trace)
- **Curriculum Agent** (LLM pipeline for outcomes/modules/assessments)
- **IR Agent** (FAISS + SentenceTransformers over `data/corpus/`)
- **NLP Agent** (spaCy NER redaction + HuggingFace summarizer)

---

## Quickstart (No Docker)

### 1. Setup virtual environment
```bash
python -m venv .venv
# Linux/Mac
source .venv/bin/activate
# Windows PowerShell
.venv\Scripts\Activate.ps1

python -m pip install --upgrade pip
```

### 2. Install dependencies
```bash
pip install -r apps/orchestrator/requirements.txt
pip install -r apps/ir_agent/requirements.txt
pip install -r apps/nlp_agent/requirements.txt
pip install -r apps/curriculum_agent/requirements.txt
```

### 3. Environment variables
Copy `.env` (already provided).  
Defaults point to `localhost` ports.

Example overrides before running a service:

**Linux/Mac**
```bash
export CURRICULUM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
export JWT_SECRET=change_me_super_secret
```

**Windows PowerShell**
```powershell
$env:CURRICULUM_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
$env:JWT_SECRET = "change_me_super_secret"
```

### 4. Run services (each in its own terminal)
- **Curriculum Agent (8001)**
```bash
cd apps/curriculum_agent
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```
- **IR Agent (8002)**
```bash
cd apps/ir_agent
uvicorn main:app --host 0.0.0.0 --port 8002 --reload
```
- **NLP Agent (8003)**
```bash
cd apps/nlp_agent
uvicorn main:app --host 0.0.0.0 --port 8003 --reload
```
- **Orchestrator (8000)**
```bash
cd apps/orchestrator
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 5. JWT for testing
Generate a JWT with payload `{"sub":"demo"}` signed using `JWT_SECRET`:

```python
from jose import jwt
print(jwt.encode({"sub":"demo"}, "change_me_super_secret", algorithm="HS256"))
```

Use in Swagger UI: `Authorization: Bearer <token>`.

---

## Endpoints

- Orchestrator: `POST /design_course`
- Curriculum Agent: `/outcomes`, `/modules`, `/assessments`
- IR Agent: `/search`, `/reindex`
- NLP Agent: `/ner_redact`, `/summarize`


> **Python 3.13 Note:** This folder is patched for Python 3.13 by removing FAISS and spaCy NER build requirements and pinning `torch==2.8.0`. IR search now uses cosine similarity via NumPy, and the NLP agent exposes only `/summarize` (NER redaction disabled). On Python 3.11, you can use the original FAISS + spaCy setup.


---

> **Default LLM:** The Curriculum Agent loads `google/gemma-2b-it` by default. Make sure your Hugging Face account has access to Gemma (`huggingface-cli login`).

> **Python 3.13:** This build removes FAISS and spaCy NER (for wheel availability). IR uses NumPy cosine similarity, and the NLP agent exposes only `/summarize`. On Python 3.11 you can enable FAISS & spaCy.

## Notes

- If `gemma-2b` is too heavy, switch `CURRICULUM_MODEL` in `.env` to a smaller one like `TinyLlama/TinyLlama-1.1B-Chat-v1.0`.
- Place your `.txt` files under `data/corpus/` for the IR agent.
- This is a skeleton for coursework — you can extend with logging, metrics, and tests.

---

### Windows PowerShell quick env
Set these **in the window where you run the service**:

```powershell
$env:CURRICULUM_MODEL = "google/gemma-2b-it"
$env:JWT_SECRET = "change_me_super_secret"
```


## Gemma‑2B Quick Notes
- Recommended: GPU with ≥8 GB VRAM. CPU works but will be slow.
- Install: `pip install transformers accelerate torch --extra-index-url https://download.pytorch.org/whl/cu121` (choose the wheel that matches your CUDA).
- First run will download ~2–3 GB of weights. Make sure you have access to **google/gemma-2b-it** on Hugging Face.
- Switch models by setting `CURRICULUM_MODEL` in `.env`.

## One‑liner demo (after all 3 agents are running)
```bash
# 1) Get a token
python scripts/make_token.py > tmp_token.txt
TOKEN=$(cat tmp_token.txt)
# 2) Call orchestrator to design a course using Gemma‑2B
curl -s -X POST http://127.0.0.1:8000/design \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "course_title": "Data Science",
    "level": "Advanced",
    "modules_count": 8,
    "total_estimated_hours": 32
  }' | jq . | tee sample_outputs.json
```

The `sample_outputs.json` will contain real outcomes/modules/assessments generated by **Gemma‑2B‑Instruct**.
